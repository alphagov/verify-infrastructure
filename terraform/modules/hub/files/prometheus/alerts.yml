---
groups:
- name: prometheus_base
  rules:
  - alert: AlwaysAlert
    annotations:
      message: |
        This is an alert meant to ensure that the entire alerting pipeline is functional.
        This alert is always firing, therefore it should always be firing in Alertmanager
        and always fire against a receiver.  We use cronitor to alert us if this ever
        *doesn't* fire, because this indicates a problem with our alerting pipeline
    expr: vector(1)
    labels:
      severity: "constant"
- name: service
  rules:
  - alert: HubSamlProxyErrorsReceivingRequest
    labels:
      severity: "p1"
    annotations:
      message: |
        It looks like users are having trouble starting sessions at
        the hub.  We expect that the saml-proxy handleRequestPost
        endpoint should return a 2xx response under normal conditions.
        We are observing the rate of 2xx responses has dropped below
        95%.
    expr: |
      sum without(instance)(
          rate(uk_gov_ida_hub_samlproxy_resources_SamlMessageReceiverApi_handleRequestPost_2xx_responses_total[1m]))
      / sum without(instance)(
          rate(uk_gov_ida_hub_samlproxy_resources_SamlMessageReceiverApi_handleRequestPost_count[1m]))
      < 0.95
    for: 15m
  - alert: HubSamlProxyErrorsReceivingResponse
    labels:
      severity: "p1"
    annotations:
      message: |
        It looks like users are having trouble returning to the hub
        having verified at an IDP.  We expect that the saml-proxy
        handleResponsePost endpoint should return a 2xx response under
        normal conditions.  We are observing the rate of 2xx responses
        has dropped below 95%.
    expr: |
      sum without(instance)(
          rate(uk_gov_ida_hub_samlproxy_resources_SamlMessageReceiverApi_handleResponsePost_2xx_responses_total[1m]))
      / sum without(instance)(
          rate(uk_gov_ida_hub_samlproxy_resources_SamlMessageReceiverApi_handleResponsePost_count[1m]))
      < 0.95
    for: 15m
  - alert: HubSamlProxyErrorsSendingRequest
    labels:
      severity: "p1"
    annotations:
      message: |
        It looks like users are hitting errors when we try to redirect
        users to an IDP that they have chosen.  We expect that the
        saml-proxy sendJsonAuthnRequestFromHub endpoint should return
        a 2xx response under normal conditions.  We are observing the
        rate of 2xx responses has dropped below 95%.
    expr: |
      sum without(instance)(
          rate(uk_gov_ida_hub_samlproxy_resources_SamlMessageSenderApi_sendJsonAuthnRequestFromHub_2xx_responses_total[1m]))
      / sum without(instance)(
          rate(uk_gov_ida_hub_samlproxy_resources_SamlMessageSenderApi_sendJsonAuthnRequestFromHub_count[1m]))
      < 0.95
    for: 15m
  - alert: HubSamlProxyErrorsSendingResponse
    labels:
      severity: "p1"
    annotations:
      message: |
        It looks like users are hitting errors when we try to finish
        their journey and redirect them back to the service they want
        to use.  We expect that the saml-proxy
        sendJsonAuthResponseFromHub endpoint should return a 2xx
        response under normal conditions.  We are observing the rate
        of 2xx responses has dropped below 95%.
    expr: |
      sum without(instance)(
          rate(uk_gov_ida_hub_samlproxy_resources_SamlMessageSenderApi_sendJsonAuthnResponseFromHub_2xx_responses_total[1m]))
      / sum without(instance)(
          rate(uk_gov_ida_hub_samlproxy_resources_SamlMessageSenderApi_sendJsonAuthnResponseFromHub_count[1m]))
      < 0.95
    for: 15m
  - alert: EveryMsaIsUnhealthy
    labels:
      severity: "p1"
    annotations:
      message: |
        We run regular MSA healthchecks.  It looks like every single
        MSA is unhealthy.  This will mean that users are unable to
        complete journeys at the hub and probably indicates a problem
        with the hub rather than with any corresponding MSA.
    expr: |
      sum(verify_saml_soap_proxy_msa_health_status) < 1
- name: infra
  rules:
  - alert: TargetDown
    labels: &infraticket
      layer: infra
      severity: ticket
    annotations:
      message: |
        A prometheus scrape target is down.
    expr: |
      up == 0
    for: 15m
  - alert: JournalbeatDown
    labels: *infraticket
    annotations:
      message: |
        beat-exporter couldn't find a running journalbeat.
    expr: |
      journalbeat_up == 0
    for: 15m
  - alert: RebootRequired
    labels: *infraticket
    annotations:
      message: |
        An instance has installed an upgrade which requires a reboot to take effect.
    expr: |
      node_reboot_required == 1
  - alert: NtpOffsetTooGreat
    labels: *infraticket
    annotations:
      message: |
        The system time is more than a second out of sync with the ntp reference server
    expr: |
      abs(node_ntp_offset_seconds) >= 1
    for: 5m
  - alert: DiskPredictedToFill
    labels: *infraticket
    annotations:
      message: |
        The instance's disk is predicted to fill within 3 days.
    expr: |
      predict_linear(
        node_filesystem_avail{
          fstype!~"squashfs|fuse[.]lxcfs"
        }[24h],
        3 * 86400) <= 0
      and on (instance) (time() - node_creation_time) > 86400
  - alert: AnalyticsHighErrorRate
    labels: *infraticket
    annotations:
      message: |
        We're seeing an elevated error rate in the analytics target group.
    expr: |
      sum(
        rate({
          __name__=~"aws_applicationelb_httpcode_target_[23]_xx_count_sum",
          target_group=~"targetgroup/.*-ingress-analytics/.*"
        }[60m]))
      / sum(
        rate({
          __name__=~"aws_applicationelb_httpcode_target_[2345]_xx_count_sum",
          target_group=~"targetgroup/.*-ingress-analytics/.*"
        }[60m])) < 0.95
    for: 15m
  - alert: NoJournalbeatLogs
    labels: *infraticket
    annotations:
      message: |
        We haven't seen any journalbeat logs for 20 minutes.
    for: 20m
    expr: |
      rate(journalbeat_libbeat_output_events{type="acked"}[1m]) == 0


